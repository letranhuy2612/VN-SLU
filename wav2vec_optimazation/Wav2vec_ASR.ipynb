{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukncmUREYjT5"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9jyjDbhtLRw"
      },
      "outputs": [],
      "source": [
        "BASE_WAV2VEC_MODEL = \"nguyenvulebinh/wav2vec2-base-vietnamese-250h\"\n",
        "BASE_WAV2VEC_PROCESSOR = BASE_WAV2VEC_MODEL\n",
        "OUTPUT_DIR= '/data'\n",
        "MY_MODEL_DIR = \"/checkpoints\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCwzEsYtYvcv"
      },
      "source": [
        "# Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH5T8hhDzV-F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import librosa\n",
        "from tqdm import tqdm\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import uuid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzDzGnsOzF2V"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "\n",
        "vocab = 'ẻ6ụí3ỹýẩởềõ7êứỏvỷalựqờjốàỗnéủуôuyằ4wbệễsìầỵ8dểrũcạ9ếùỡ2tiǵử̀á0ậeộmẳợĩhâúọồặfữắỳxóãổị̣zảđèừòẵ1ơkẫpấẽỉớẹăoư5|'\n",
        "def clear_text(row):\n",
        "  correct = [\n",
        "    ['%', ' phần trăm '],\n",
        "\n",
        "  ]\n",
        "\n",
        "  text = row['text'].lower()\n",
        "  for item in correct:\n",
        "    text = text.replace(item[0], item[1])\n",
        "\n",
        "  text = re.sub('[^' + vocab + ']', ' ', text).strip()\n",
        "  text = ' '.join(text.split())\n",
        "\n",
        "  row['text'] = text\n",
        "  return row\n",
        "\n",
        "#Create vocab+tokenizer+processor\n",
        "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"\\“\\%\\‘\\”\\�]'\n",
        "\n",
        "def remove_special_characters(batch):\n",
        "    batch[\"text\"] = re.sub(chars_to_ignore_regex, '', batch[\"text\"]).lower()\n",
        "    return batch\n",
        "\n",
        "def extract_all_chars(batch):\n",
        "  all_text = \" \".join(batch[\"text\"])\n",
        "  vocab = list(set(all_text))\n",
        "  return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
        "\n",
        "def create_vocab(train_ds, test_ds, vocab_json):\n",
        "  train_ds = train_ds.map(remove_special_characters)\n",
        "  test_ds = test_ds.map(remove_special_characters)\n",
        "\n",
        "  vocab_train = train_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=train_ds.column_names)\n",
        "  vocab_test = test_ds.map(extract_all_chars, batched=True, batch_size=-1, keep_in_memory=True, remove_columns=test_ds.column_names)\n",
        "\n",
        "  vocab_list = list(set(vocab_train[\"vocab\"][0]) | set(vocab_test[\"vocab\"][0]))\n",
        "  vocab_dict = {v: k for k, v in enumerate(vocab_list)}\n",
        "\n",
        "  vocab_dict[\"|\"] = vocab_dict[\" \"]\n",
        "  del vocab_dict[\" \"]\n",
        "  vocab_dict[\"[UNK]\"] = len(vocab_dict)\n",
        "  vocab_dict[\"[PAD]\"] = len(vocab_dict)\n",
        "\n",
        "  with open(vocab_json, 'w') as vocab_file:\n",
        "    json.dump(vocab_dict, vocab_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HCpTxZI0XEa"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    AutoProcessor,\n",
        "    Wav2Vec2Processor,\n",
        "    Wav2Vec2ProcessorWithLM,\n",
        "    Wav2Vec2CTCTokenizer,\n",
        "    Wav2Vec2FeatureExtractor,\n",
        "    Wav2Vec2ForCTC,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "def create_tokenizer(model_path, train_ds, test_ds):\n",
        "  try:\n",
        "    tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(model_path, do_lower_case=True)\n",
        "  except:\n",
        "    vocab_json = \"./tmp_vocab.json\"\n",
        "    create_vocab(train_ds, test_ds, vocab_json)\n",
        "\n",
        "    tokenizer = Wav2Vec2CTCTokenizer(vocab_json, unk_token=\"[UNK]\", pad_token=\"[PAD]\", word_delimiter_token=\"|\")\n",
        "  return tokenizer\n",
        "\n",
        "def create_processor(model_path):\n",
        "  try:\n",
        "    processor = AutoProcessor.from_pretrained(model_path)\n",
        "  except:\n",
        "    feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_path)\n",
        "    tokenizer = create_tokenizer(model_path)\n",
        "    processor = Wav2Vec2Processor(feature_extractor, tokenizer)\n",
        "  return processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-tLb4dg0ZCw"
      },
      "outputs": [],
      "source": [
        "import jsonlines\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "label_path = 'label.jsonl'\n",
        "results = []\n",
        "with jsonlines.open(label_path) as jsonl_file:\n",
        "    for line in jsonl_file:\n",
        "      results.append({\"file\": 'D:/Downloads/SLU/SLU/train/data/'+line[\"file\"],\"text\": line[\"sentence\"]})\n",
        "df = pd.DataFrame(results)\n",
        "df = df[df[\"text\"].str.find('???') == -1]\n",
        "df = df.apply(clear_text, axis=1)\n",
        "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ifnXb-NS6wSe"
      },
      "outputs": [],
      "source": [
        "train_df.to_csv(os.path.join(OUTPUT_DIR, \"train_df.csv\"), index=False)\n",
        "test_df.to_csv(os.path.join(OUTPUT_DIR, \"test_df.csv\"), index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Nvaejqh34HZ"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "\n",
        "## load train dataset from pre-step\n",
        "train_ds = datasets.load_dataset('csv', data_files=os.path.join(OUTPUT_DIR, \"train_df.csv\"), keep_in_memory=True, split='train')\n",
        "test_ds = datasets.load_dataset('csv', data_files=os.path.join(OUTPUT_DIR, \"test_df.csv\"), keep_in_memory=True, split='train')\n",
        "processor = create_processor(BASE_WAV2VEC_PROCESSOR)\n",
        "def speech_file_to_array_fn(batch):\n",
        "  speech_array, sampling_rate = sf.read(batch[\"file\"])\n",
        "  batch[\"input_values\"] = processor(speech_array, sampling_rate=sampling_rate).input_values[0]\n",
        "  with processor.as_target_processor():\n",
        "      batch[\"labels\"] = processor(batch[\"text\"]).input_ids\n",
        "  return batch\n",
        "train_ds = train_ds.map(speech_file_to_array_fn, remove_columns=train_ds.column_names)\n",
        "test_ds = test_ds.map(speech_file_to_array_fn, remove_columns=test_ds.column_names)\n",
        "train_ds.save_to_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"train\"))\n",
        "test_ds.save_to_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"test\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpwlUuP8Y6ak"
      },
      "source": [
        "# Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taMXNHpW5jot"
      },
      "source": [
        "## Distilling Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JI-g--wwMCb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klIKDUSYFDxG"
      },
      "outputs": [],
      "source": [
        "model = Wav2Vec2ForCTC.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "processor = Wav2Vec2Processor.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "print(\"Number of teacher model parameters: \", model.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHWQCTgQG47A"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU8UBOJjEyBi"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def deleteEncodingLayers(model, num_layers_to_keep):\n",
        "    oldModuleList = model.wav2vec2.encoder.layers\n",
        "    newModuleList = nn.ModuleList()\n",
        "\n",
        "    for i in range(0, num_layers_to_keep):\n",
        "        newModuleList.append(oldModuleList[i])\n",
        "\n",
        "    model.wav2vec2.encoder.layers = newModuleList\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrl1uEHmGx1o"
      },
      "outputs": [],
      "source": [
        "### Load distilled model from checkpoint\n",
        "distilled_wav2vec2 = Wav2Vec2ForCTC.from_pretrained(MY_MODEL_DIR)\n",
        "distilled_wav2vec2 = deleteEncodingLayers(distilled_wav2vec2, 6)\n",
        "print(\"Number of teacher model parameters: \", distilled_wav2vec2.num_parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kL-qhGvnIXXA"
      },
      "outputs": [],
      "source": [
        "distilled_wav2vec2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqj5BZqj5m6N"
      },
      "source": [
        "## Distillation Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJjZrtjxNyrq"
      },
      "outputs": [],
      "source": [
        "train_ds = datasets.load_from_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"train\"))\n",
        "test_ds = datasets.load_from_disk(os.path.join(OUTPUT_DIR, \"hf_datastet\", \"test\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nhWuMW1OA-z"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_ds.set_format(\"torch\")\n",
        "test_ds.set_format(\"torch\")\n",
        "train_dataloader = DataLoader(train_ds, shuffle=True, batch_size=1)\n",
        "eval_dataloader = DataLoader(test_ds, batch_size=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gujbpJh4OG1E"
      },
      "outputs": [],
      "source": [
        "from torch.optim import AdamW\n",
        "from transformers import get_scheduler\n",
        "\n",
        "optimizer = AdamW(distilled_wav2vec2.parameters(), lr=5e-5)\n",
        "\n",
        "epochs = 15\n",
        "num_training_steps = epochs * len(train_dataloader)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jU0XcR1oqWgt"
      },
      "source": [
        "$$ L_{\\text{distil}} = KLLoss(\\sigma(z_{student}/T) , \\sigma(z_{teacher}/T)) * T^2 $$\n",
        "\n",
        "$$ L_{\\text{final}} = \\alpha L_{\\text{distil}} + (1 - \\alpha) L_{\\text{student}} $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc6_676vq4ND"
      },
      "outputs": [],
      "source": [
        "T = 4\n",
        "alpha = 0.8\n",
        "\n",
        "def compute_distil_loss(student_outputs, teacher_outputs):\n",
        "  kl_loss = torch.nn.KLDivLoss()\n",
        "  student_logits = student_outputs.logits\n",
        "  teacher_logits = teacher_outputs.logits\n",
        "  distil_loss = kl_loss(\n",
        "      F.log_softmax(student_logits/T, dim=1),\n",
        "      F.softmax(teacher_logits/T, dim=1)\n",
        "  ) * T * T\n",
        "  return distil_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC9o05DWP4w6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "teacher_model = Wav2Vec2ForCTC.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "teacher_model.eval()\n",
        "teacher_model.to(device)\n",
        "distilled_wav2vec2.to(device)\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "distilled_wav2vec2.train()\n",
        "for epoch in range(epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        student_outputs = distilled_wav2vec2(**batch)\n",
        "        teacher_outputs = teacher_model(**batch)\n",
        "\n",
        "        distil_loss = compute_distil_loss(student_outputs, teacher_outputs)\n",
        "        student_loss = student_outputs.loss\n",
        "        final_loss = alpha * distil_loss + (1. - alpha) * student_loss\n",
        "        final_loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "    distilled_wav2vec2.save_pretrained(save_directory=MY_MODEL_DIR)\n",
        "    processor.save_pretrained(save_directory=MY_MODEL_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A435cPqNcvkZ"
      },
      "source": [
        "## Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xt3RzNigtpfW"
      },
      "outputs": [],
      "source": [
        "# !apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev\n",
        "# !wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz\n",
        "# !mkdir kenlm/build && cd kenlm/build && cmake .. && make -j2\n",
        "# !ls kenlm/build/bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iGTht3u7ujOs"
      },
      "outputs": [],
      "source": [
        "#! kenlm/build/bin/lmplz -o 4 <\"/content/all_text.txt\" > \"4gram.arpa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-DDiu8neRbR"
      },
      "outputs": [],
      "source": [
        "import kenlm\n",
        "from pyctcdecode import Alphabet, BeamSearchDecoderCTC, LanguageModel\n",
        "\n",
        "def get_decoder_ngram_model(tokenizer, ngram_lm_path):\n",
        "    vocab_dict = tokenizer.get_vocab()\n",
        "    sort_vocab = sorted((value, key) for (key, value) in vocab_dict.items())\n",
        "    vocab = [x[1] for x in sort_vocab][:-2]\n",
        "    vocab_list = vocab\n",
        "    # convert ctc blank character representation\n",
        "    vocab_list[tokenizer.pad_token_id] = \"\"\n",
        "    # replace special characters\n",
        "    vocab_list[tokenizer.unk_token_id] = \"\"\n",
        "    # vocab_list[tokenizer.bos_token_id] = \"\"\n",
        "    # vocab_list[tokenizer.eos_token_id] = \"\"\n",
        "    # convert space character representation\n",
        "    vocab_list[tokenizer.word_delimiter_token_id] = \" \"\n",
        "    # specify ctc blank char index, since conventially it is the last entry of the logit matrix\n",
        "    alphabet = Alphabet.build_alphabet(vocab_list, ctc_token_idx=tokenizer.pad_token_id)\n",
        "    lm_model = kenlm.Model(ngram_lm_path)\n",
        "    decoder = BeamSearchDecoderCTC(alphabet,\n",
        "                                   language_model=LanguageModel(lm_model))\n",
        "    return decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rW55amueSt-"
      },
      "outputs": [],
      "source": [
        "my_lm_file = os.path.join(MY_MODEL_DIR, \"lm_4.arpa\")\n",
        "ngram_lm_model = get_decoder_ngram_model(processor.tokenizer, my_lm_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfOPymNb5JzZ"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp8MNEko5RMB"
      },
      "source": [
        "## Teacher"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMdzUz5B5I9j"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from datasets import load_metric\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "teacher_model = Wav2Vec2ForCTC.from_pretrained(BASE_WAV2VEC_MODEL)\n",
        "teacher_model.eval()\n",
        "teacher_model.to(device)\n",
        "\n",
        "def map_to_result(batch):\n",
        "    with torch.no_grad():\n",
        "        input_values = torch.tensor(batch[\"input_values\"], device=device).unsqueeze(0)\n",
        "        outputs = teacher_model(input_values)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    # batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
        "    batch[\"pred_str_lm\"] = ngram_lm_model.decode(logits[0].cpu().detach().numpy(), beam_width=64)\n",
        "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
        "    batch[\"outputs\"] = outputs\n",
        "    batch[\"logits\"] = logits\n",
        "\n",
        "    return batch\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "teacher_results = test_ds.map(map_to_result, remove_columns=test_ds.column_names)\n",
        "print(\"Inference time: {:.3f}\".format(time.perf_counter() - start_time))\n",
        "\n",
        "wer_metric = load_metric(\"wer\")\n",
        "# print(\"Test WER without LM: {:.3f}\".format(wer_metric.compute(predictions=teacher_results[\"pred_str\"], references=teacher_results[\"text\"])))\n",
        "print(\"Test WER with LM: {:.3f}\".format(wer_metric.compute(predictions=teacher_results[\"pred_str_lm\"], references=teacher_results[\"text\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mNUu_OwsqpR"
      },
      "outputs": [],
      "source": [
        "teacher_model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut_E1kNr5N-N"
      },
      "outputs": [],
      "source": [
        "teacher_results[\"pred_str\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCRScUuNl1sS"
      },
      "outputs": [],
      "source": [
        "teacher_results[\"pred_str_lm\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyC5LR595O4a"
      },
      "outputs": [],
      "source": [
        "teacher_results[\"text\"][:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9xitX9_5UBX"
      },
      "source": [
        "## Student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sreX86wU4mbe"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
        "from datasets import load_metric\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "distilled_wav2vec2.eval()\n",
        "distilled_wav2vec2.to(device)\n",
        "# processor = Wav2Vec2Processor.from_pretrained(MY_MODEL_DIR)\n",
        "\n",
        "def map_to_result(batch):\n",
        "    with torch.no_grad():\n",
        "        input_values = torch.tensor(batch[\"input_values\"], device=device).unsqueeze(0)\n",
        "        outputs = distilled_wav2vec2(input_values)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    pred_ids = torch.argmax(logits, dim=-1)\n",
        "    batch[\"pred_str\"] = processor.batch_decode(pred_ids)[0]\n",
        "    batch[\"pred_str_lm\"] = ngram_lm_model.decode(logits[0].cpu().detach().numpy(), beam_width=64)\n",
        "    batch[\"text\"] = processor.decode(batch[\"labels\"], group_tokens=False)\n",
        "    batch[\"outputs\"] = outputs\n",
        "    batch[\"logits\"] = logits\n",
        "\n",
        "    return batch\n",
        "\n",
        "start_time = time.perf_counter()\n",
        "student_results = test_ds.map(map_to_result, remove_columns=test_ds.column_names)\n",
        "print(\"Inference time: {:.3f}\".format(time.perf_counter() - start_time))\n",
        "\n",
        "wer_metric = load_metric(\"wer\")\n",
        "print(\"Test WER without LM: {:.3f}\".format(wer_metric.compute(predictions=student_results[\"pred_str\"], references=student_results[\"text\"])))\n",
        "print(\"Test WER with LM: {:.3f}\".format(wer_metric.compute(predictions=student_results[\"pred_str_lm\"], references=student_results[\"text\"])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cZyGmzGxeVyM"
      },
      "outputs": [],
      "source": [
        "distilled_wav2vec2.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WZ3BDgt5NuG"
      },
      "outputs": [],
      "source": [
        "student_results[\"pred_str\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lld7rm3Pmk52"
      },
      "outputs": [],
      "source": [
        "student_results[\"pred_str_lm\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1jpGtTM5Obs"
      },
      "outputs": [],
      "source": [
        "student_results[\"text\"][:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrFHUQt7mv9h"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "test_csv = os.path.join(OUTPUT_DIR, 'test_df.csv')\n",
        "audio_files = []\n",
        "with open(test_csv, newline='') as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "    for row in reader:\n",
        "        audio_files.append(row[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Fiu9IqipXbL"
      },
      "outputs": [],
      "source": [
        "audio_files[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_onnx(model, onnx_model_path):\n",
        "    print(f\"Converting model to onnx\")\n",
        "\n",
        "    audio_len = 250000\n",
        "    x = torch.randn(1, audio_len, requires_grad=True)\n",
        "\n",
        "    torch.onnx.export(model,                        # model being run\n",
        "                    x,                              # model input (or a tuple for multiple inputs)\n",
        "                    onnx_model_path,                # where to save the model (can be a file or file-like object)\n",
        "                    export_params=True,             # store the trained parameter weights inside the model file\n",
        "                    opset_version=11,               # the ONNX version to export the model to\n",
        "                    do_constant_folding=True,       # whether to execute constant folding for optimization\n",
        "                    input_names = ['input'],        # the model's input names\n",
        "                    output_names = ['output'],      # the model's output names\n",
        "                    dynamic_axes={'input' : {1 : 'audio_len'},    # variable length axes\n",
        "                                'output' : {1 : 'audio_len'}})\n",
        "\n",
        "def quantize_onnx_model(onnx_model_path, quantized_model_path):\n",
        "    print(\"Starting quantization...\")\n",
        "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "    quantize_dynamic(onnx_model_path,\n",
        "                     quantized_model_path,\n",
        "                     weight_type=QuantType.QUInt8)\n",
        "\n",
        "    print(f\"Quantized model saved to: {quantized_model_path}\")\n",
        "\n",
        "quantize = False\n",
        "onnx_model_path = os.path.join(MY_MODEL_DIR, \"wav2vec.onnx\")\n",
        "convert_to_onnx(distilled_wav2vec2, onnx_model_path)\n",
        "if (quantize):\n",
        "    quantized_model_name = os.path.join(MY_MODEL_DIR, \"wav2vec.quant.onnx\")\n",
        "    quantize_onnx_model(onnx_model_path, quantized_model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NCwzEsYtYvcv",
        "taMXNHpW5jot",
        "oqj5BZqj5m6N",
        "zp8MNEko5RMB"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
